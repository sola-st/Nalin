{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Created on 21-April-2020\n",
    "@author Jibesh Patra\n",
    "\n",
    "\"\"\"\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "from multiprocessing import cpu_count\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import fileutils as fs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### This Jupyter Notebook Specific Setup\n",
    "\n",
    "The following configuration is meant only for running this Jupyter notebook. One may use _run_classification.py_ to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "running_as_notebook = False\n",
    "root_dir = './'\n",
    "try:\n",
    "    cfg = get_ipython().config\n",
    "    running_as_notebook = True\n",
    "except NameError:\n",
    "    pass\n",
    "if running_as_notebook:\n",
    "    from collections import namedtuple\n",
    "    cur_dir = !pwd\n",
    "    root_dir = '/'.join(cur_dir[0].split('/')[:-2])\n",
    "    args = {\n",
    "        'batch_size': 128,\n",
    "        'num_epochs': 15,\n",
    "        'train': True,\n",
    "        'pos_dataset': f'{root_dir}/results/positive_examples.pkl',\n",
    "        'neg_dataset': f'{root_dir}/results/negative_examples.pkl',\n",
    "        'test': False,\n",
    "        'test_dataset': f'{root_dir}/results/test_examples.pkl',\n",
    "        'saved_model': f'{root_dir}/results/saved_models/VarValueClassifierRNN_all_types_17-11-2020--19:06:51_0.89.pt',\n",
    "        'name': 'nalin',\n",
    "        'ablation': [] # Possible values --> 'value_as_one_hot', 'var', 'type', 'len', 'shape'\n",
    "    }\n",
    "    results_dir = f'{root_dir}/results'\n",
    "    token_embedding_path = f'{root_dir}/benchmark/python_embeddings.bin'\n",
    "    positive_examples_dir = f'{root_dir}/results/dynamic_analysis_outputs'\n",
    "    list_of_types_in_dataset_out_file = f'{root_dir}/results/list_of_types_in_dataset.json'\n",
    "    Args = namedtuple('Args', args)\n",
    "    args = Args(**args)\n",
    "else:\n",
    "    from command_line_args import get_parsed_args\n",
    "    args = get_parsed_args(argparse=argparse)\n",
    "    positive_examples_dir = 'results/dynamic_analysis_outputs'\n",
    "    token_embedding_path = 'benchmark/python_embeddings.bin'\n",
    "    list_of_types_in_dataset_out_file = 'results/list_of_types_in_dataset.json'\n",
    "    results_dir = 'results'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Dataset utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from dataset_utils.data_transformers.AblationTransformer import AblationTransformer\n",
    "from dataset_utils.data_transformers.ResizeData import ResizeData\n",
    "from dataset_utils.data_transformers.ValueToCharSequence import ValueToCharSequence\n",
    "from dataset_utils.data_transformers.fastTextEmbeddingOfVarName import fastTextEmbeddingOfVarName\n",
    "from dataset_utils.data_transformers.RepresentLen import RepresentLen\n",
    "from dataset_utils.data_transformers.RepresentShape import RepresentShape\n",
    "from dataset_utils.data_transformers.OneHotEncodingOfTypes import OneHotEncodingOfType\n",
    "from dataset_utils.pre_process_dataset import process, write_types_and_frequencies\n",
    "from read_dataset import get_training_val_dataset, get_test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from models.VarValueClassifierRNN import VarValueClassifierRNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train, test = args.train, args.test\n",
    "if not train and not test:\n",
    "    print('Either \"training\" or \"testing\" is required')\n",
    "    sys.exit(1)\n",
    "\n",
    "batch_size = args.batch_size\n",
    "num_epochs = args.num_epochs\n",
    "max_num_of_chars_in_value = 100  # Number of characters in the value part of the assignment\n",
    "print(f\"-- Resizing the values to {max_num_of_chars_in_value} characters during training\")\n",
    "\n",
    "# You may specify your name\n",
    "if args.name:\n",
    "    model_name_suffix = args.name\n",
    "else:\n",
    "    model_name_suffix = 'Nalin'\n",
    "\n",
    "model_name = f'RNNClassifier_{model_name_suffix}'\n",
    "\n",
    "\n",
    "pos_dataset_file_path = args.pos_dataset\n",
    "neg_dataset_file_path = args.neg_dataset\n",
    "test_dataset_file_path = args.test_dataset\n",
    "\n",
    "\"\"\"\n",
    "There are three heuristics for generating negative examples:\n",
    "    1. use_dimension: refers to computing various properties on the positive examples and then using them to\n",
    "    generate the negative examples. (Code adapted from the initial code by MP)\n",
    "    2. random: only useful for cases when the data contains single type (eg.string). The approach is simply randomizes the\n",
    "    values. The idea is to check if certain idenfiers such as URL are only assigned values having certain properties\n",
    "    3. weighted_random: This is the default strategy. Refer to the code where it is implemented for further details.\n",
    "\"\"\"\n",
    "heuristics_for_generating_negative_examples = ['random','weighted_random'][1]\n",
    "\n",
    "# Types and the corresponding frequency in the dataset\n",
    "\"\"\"\n",
    "Pre-process dataset. This is an one time task ==>\n",
    "    - Remove empty/malformed extracted data\n",
    "    - Create negative examples\n",
    "    - Create labels for the extracted data (label -> probability of buggy)\n",
    "\"\"\"\n",
    "if not test:\n",
    "    process(positive_examples_dir=positive_examples_dir,\n",
    "            positive_example_out_file_path=pos_dataset_file_path,\n",
    "            negative_example_out_file_path=neg_dataset_file_path,\n",
    "            test_example_out_file_path=test_dataset_file_path,\n",
    "            heuristics_for_generating_negative_examples=heuristics_for_generating_negative_examples)\n",
    "    write_types_and_frequencies(positive_example_out_file_path=pos_dataset_file_path,\n",
    "                                list_of_types_in_dataset_out_file=list_of_types_in_dataset_out_file)\n",
    "\n",
    "# Embeddings have been learned from ALL python files in the benchmark (~1M files). We could\n",
    "# successfully extract assignments from some of these python files.\n",
    "\n",
    "if not os.path.exists(token_embedding_path):\n",
    "    print(f'Could not read from {token_embedding_path}. \\nNeed an embedding path to continue')\n",
    "    sys.exit(1)\n",
    "test_examples_dir = 'results/test_examples'\n",
    "saved_model_path = None\n",
    "if args.test and args.saved_model:\n",
    "    saved_model_path = args.saved_model\n",
    "elif args.test and not args.saved_model:\n",
    "    print(\"A saved model path is needed\")\n",
    "    sys.exit(1)\n",
    "embedding_dim = 0\n",
    "features_to_ablate = args.ablation\n",
    "\n",
    "\n",
    "# Workaround for debugging on a laptop. Change with the cpu_count of your machine if required for debugging data loading\n",
    "# else leave it alone\n",
    "if cpu_count() > 20:\n",
    "    num_workers_for_data_loading = cpu_count()\n",
    "else:\n",
    "    num_workers_for_data_loading = 0\n",
    "config = {\"num_workers\": num_workers_for_data_loading, \"pin_memory\": True}\n",
    "\n",
    "device = torch.device(\n",
    "    'cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Initialize model and model specific dataset data_transformers\n",
    "print(f\"\\n{'-' * 20} Using model '{model_name}' {'-' * 20}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Data Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "resize_data = ResizeData(len_of_value=max_num_of_chars_in_value)\n",
    "value_to_one_hot = ValueToCharSequence(\n",
    "    len_of_value=max_num_of_chars_in_value)\n",
    "\n",
    "one_hot_encoding_of_type = OneHotEncodingOfType(max_types_to_select=10,\n",
    "                                                types_in_dataset_file_path=list_of_types_in_dataset_out_file)  # We select only top 10 types\n",
    "size_of_type_encoding = len(one_hot_encoding_of_type.one_hot_init)\n",
    "\n",
    "var_name_fastText_embd = fastTextEmbeddingOfVarName(embedding_path=token_embedding_path)\n",
    "embedding_dim = var_name_fastText_embd.embedding_dim\n",
    "\n",
    "len_repr = RepresentLen()\n",
    "shape_repr = RepresentShape()\n",
    "\n",
    "data_transformations = [resize_data,  # must be always the first transformation\n",
    "                        var_name_fastText_embd,\n",
    "                        value_to_one_hot,\n",
    "                        one_hot_encoding_of_type,\n",
    "                        len_repr,\n",
    "                        shape_repr\n",
    "                        ]\n",
    "\n",
    "model = VarValueClassifierRNN(embedding_dim=embedding_dim,\n",
    "                              num_of_characters_in_alphabet=value_to_one_hot.nbs_chars,\n",
    "                              model_name=model_name,\n",
    "                              size_of_value=resize_data.len_of_value)\n",
    "\n",
    "assert model is not None, \"Initialize a model to run training/testing\"\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Ablation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if len(features_to_ablate):\n",
    "    ablation_transformer = AblationTransformer(features_to_ablate=features_to_ablate)\n",
    "    print(f\"## Not using features --> {features_to_ablate} ##\")\n",
    "    data_transformations.append(ablation_transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if train:\n",
    "    print(f\"{'-' * 15} Reading dataset for training {'-' * 15}\")\n",
    "    # Read the dataset\n",
    "    training_dataset, validation_dataset = get_training_val_dataset(\n",
    "        positive_examples_dataset_file_path=pos_dataset_file_path,\n",
    "        negative_examples_dataset_file_path=neg_dataset_file_path,\n",
    "        all_transformations=data_transformations,\n",
    "        nb_examples=-1)\n",
    "    train_data = DataLoader(\n",
    "        dataset=training_dataset, batch_size=batch_size, shuffle=True, drop_last=True, **config)\n",
    "    validation_data = DataLoader(\n",
    "        dataset=validation_dataset, batch_size=batch_size, shuffle=True, drop_last=True, **config)\n",
    "\n",
    "    model.run_epochs(training_data=train_data,\n",
    "                     validation_data=validation_data, num_epochs=num_epochs, results_dir=results_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if test:\n",
    "    print(f\"{'-' * 15} Reading dataset for testing {'-' * 15}\")\n",
    "    test_dataset = get_test_dataset(\n",
    "        test_examples_dir=test_examples_dir,\n",
    "        results_dir=results_dir,\n",
    "        all_transformations=data_transformations,\n",
    "        dataset_out_file=test_dataset_file_path)\n",
    "    batched_test_dataset = DataLoader(\n",
    "        dataset=test_dataset, batch_size=batch_size, shuffle=False, drop_last=False, **config)\n",
    "    model.load_model(path_to_saved_model=saved_model_path)\n",
    "    predictions = model.run_testing(data=batched_test_dataset)\n",
    "\n",
    "    test_data_with_predictions = test_dataset.data\n",
    "    test_data_with_predictions['predicted_p_buggy'] = predictions\n",
    "    fs.create_dir_list_if_not_present([os.path.join(results_dir, f'prediction_results')])\n",
    "    predicted_outfile_path = os.path.join(results_dir,\n",
    "                                          f'prediction_results/{Path(test_dataset_file_path).stem}_predictions.pkl')\n",
    "    print(f\"Writing to '{predicted_outfile_path}'\")\n",
    "    test_data_with_predictions.sort_values('predicted_p_buggy', ascending=False, inplace=True)\n",
    "    test_data_with_predictions.reset_index(drop=True, inplace=True)\n",
    "    # print(\n",
    "    #     f\"\\n Prediction results is follows: \\n\\n{test_data['predicted_p_buggy'].value_counts()}\")\n",
    "\n",
    "    # test_data_with_predictions.to_csv(predicted_outfile_path)\n",
    "    test_data_with_predictions.to_pickle(path=predicted_outfile_path, compression='gzip')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f2ac3815f490baa7abc22c2717d4dc21d64c0b7e1df0d49128b5f181d05973e7"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
