{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Created on 21-April-2020\n",
    "@author Jibesh Patra\n",
    "\n",
    "\"\"\"\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "from multiprocessing import cpu_count\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### This Jupyter Notebook Specific Setup\n",
    "\n",
    "The following configuration is meant only for running this Jupyter notebook. One may use _run_classification.py_ to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "running_as_notebook = False\n",
    "root_dir = './'\n",
    "try:\n",
    "    cfg = get_ipython().config\n",
    "    running_as_notebook = True\n",
    "except NameError:\n",
    "    pass\n",
    "if running_as_notebook:\n",
    "    from collections import namedtuple\n",
    "    cur_dir = !pwd\n",
    "    root_dir = '/'.join(cur_dir[0].split('/')[:-2])\n",
    "    args = {\n",
    "        'batch_size': 128,\n",
    "        'num_epochs': 15,\n",
    "        'train': True,\n",
    "        'pos_dataset': f'{root_dir}/results/positive_examples.pkl',\n",
    "        'neg_dataset': f'{root_dir}/results/negative_examples.pkl',\n",
    "        'test': False,\n",
    "        'test_dataset': f'{root_dir}/results/test_examples.pkl',\n",
    "        'saved_model': f'{root_dir}/results/saved_models/VarValueClassifierRNN_all_types_17-11-2020--19:06:51_0.89.pt',\n",
    "        'name': 'nalin',\n",
    "        'ablation': [] # Possible values --> 'value_as_one_hot', 'var', 'type', 'len', 'shape'\n",
    "    }\n",
    "    results_dir = f'{root_dir}/results'\n",
    "    token_embedding_path = f'{root_dir}/benchmark/python_embeddings.bin'\n",
    "    positive_examples_dir = f'{root_dir}/results/dynamic_analysis_outputs'\n",
    "    list_of_types_in_dataset_out_file = f'{root_dir}/results/list_of_types_in_dataset.json'\n",
    "    Args = namedtuple('Args', args)\n",
    "    args = Args(**args)\n",
    "else:\n",
    "    from command_line_args import get_parsed_args\n",
    "    args = get_parsed_args(argparse=argparse)\n",
    "    positive_examples_dir = 'results/dynamic_analysis_outputs'\n",
    "    token_embedding_path = 'benchmark/python_embeddings.bin'\n",
    "    list_of_types_in_dataset_out_file = 'results/list_of_types_in_dataset.json'\n",
    "    results_dir = 'results'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Dataset utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from dataset_utils.data_transformers.AblationTransformer import AblationTransformer\n",
    "from dataset_utils.data_transformers.ResizeData import ResizeData\n",
    "from dataset_utils.data_transformers.ValueToCharSequence import ValueToCharSequence\n",
    "from dataset_utils.data_transformers.fastTextEmbeddingOfVarName import fastTextEmbeddingOfVarName\n",
    "from dataset_utils.data_transformers.RepresentLen import RepresentLen\n",
    "from dataset_utils.data_transformers.RepresentShape import RepresentShape\n",
    "from dataset_utils.data_transformers.OneHotEncodingOfTypes import OneHotEncodingOfType\n",
    "from dataset_utils.pre_process_dataset import process, write_types_and_frequencies\n",
    "from read_dataset import get_training_val_dataset, get_test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from models.VarValueClassifierRNN import VarValueClassifierRNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Resizing the values to 100 characters during training\n",
      "Reading '/home/jibesh/nalin/results/positive_examples.pkl'\n",
      "Writing to /home/jibesh/nalin/results/list_of_types_in_dataset.json the types and corresponding frequencies\n",
      "\n",
      "-------------------- Using model 'RNNClassifier_nalin' --------------------\n"
     ]
    }
   ],
   "source": [
    "train, test = args.train, args.test\n",
    "if not train and not test:\n",
    "    print('Either \"training\" or \"testing\" is required')\n",
    "    sys.exit(1)\n",
    "\n",
    "batch_size = args.batch_size\n",
    "num_epochs = args.num_epochs\n",
    "max_num_of_chars_in_value = 100  # Number of characters in the value part of the assignment\n",
    "print(f\"-- Resizing the values to {max_num_of_chars_in_value} characters during training\")\n",
    "\n",
    "# You may specify your name\n",
    "if args.name:\n",
    "    model_name_suffix = args.name\n",
    "else:\n",
    "    model_name_suffix = 'Nalin'\n",
    "\n",
    "model_name = f'RNNClassifier_{model_name_suffix}'\n",
    "\n",
    "\n",
    "pos_dataset_file_path = args.pos_dataset\n",
    "neg_dataset_file_path = args.neg_dataset\n",
    "test_dataset_file_path = args.test_dataset\n",
    "\n",
    "\"\"\"\n",
    "There are three heuristics for generating negative examples:\n",
    "    1. use_dimension: refers to computing various properties on the positive examples and then using them to\n",
    "    generate the negative examples. (Code adapted from the initial code by MP)\n",
    "    2. random: only useful for cases when the data contains single type (eg.string). The approach is simply randomizes the\n",
    "    values. The idea is to check if certain idenfiers such as URL are only assigned values having certain properties\n",
    "    3. weighted_random: This is the default strategy. Refer to the code where it is implemented for further details.\n",
    "\"\"\"\n",
    "heuristics_for_generating_negative_examples = ['random','weighted_random'][1]\n",
    "\n",
    "# Types and the corresponding frequency in the dataset\n",
    "\"\"\"\n",
    "Pre-process dataset. This is an one time task ==>\n",
    "    - Remove empty/malformed extracted data\n",
    "    - Create negative examples\n",
    "    - Create labels for the extracted data (label -> probability of buggy)\n",
    "\"\"\"\n",
    "if not test:\n",
    "    process(positive_examples_dir=positive_examples_dir,\n",
    "            positive_example_out_file_path=pos_dataset_file_path,\n",
    "            negative_example_out_file_path=neg_dataset_file_path,\n",
    "            test_example_out_file_path=test_dataset_file_path,\n",
    "            heuristics_for_generating_negative_examples=heuristics_for_generating_negative_examples)\n",
    "    write_types_and_frequencies(positive_example_out_file_path=pos_dataset_file_path,\n",
    "                                list_of_types_in_dataset_out_file=list_of_types_in_dataset_out_file)\n",
    "\n",
    "# Embeddings have been learned from ALL python files in the benchmark (~1M files). We could\n",
    "# successfully extract assignments from some of these python files.\n",
    "\n",
    "if not os.path.exists(token_embedding_path):\n",
    "    print(f'Could not read from {token_embedding_path}. \\nNeed an embedding path to continue')\n",
    "    sys.exit(1)\n",
    "test_examples_dir = 'results/test_examples'\n",
    "saved_model_path = None\n",
    "if args.test and args.saved_model:\n",
    "    saved_model_path = args.saved_model\n",
    "elif args.test and not args.saved_model:\n",
    "    print(\"A saved model path is needed\")\n",
    "    sys.exit(1)\n",
    "embedding_dim = 0\n",
    "features_to_ablate = args.ablation\n",
    "\n",
    "\n",
    "# Workaround for debugging on a laptop. Change with the cpu_count of your machine if required for debugging data loading\n",
    "# else leave it alone\n",
    "if cpu_count() > 20:\n",
    "    num_workers_for_data_loading = cpu_count()\n",
    "else:\n",
    "    num_workers_for_data_loading = 0\n",
    "config = {\"num_workers\": num_workers_for_data_loading, \"pin_memory\": True}\n",
    "\n",
    "device = torch.device(\n",
    "    'cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Initialize model and model specific dataset data_transformers\n",
    "print(f\"\\n{'-' * 20} Using model '{model_name}' {'-' * 20}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Data Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "data": {
      "text/plain": "VarValueClassifierRNN(\n  (criterion): BCELoss()\n  (RNN_over_value): GRU(101, 100, batch_first=True, bidirectional=True)\n  (convNetVal): Sequential(\n    (0): Conv1d(101, 100, kernel_size=(100,), stride=(1,))\n    (1): ReLU()\n    (2): MaxPool1d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n  )\n  (classifier): Sequential(\n    (0): Dropout(p=0.5, inplace=False)\n    (1): Linear(in_features=612, out_features=150, bias=True)\n    (2): ReLU()\n    (3): Dropout(p=0.5, inplace=False)\n    (4): Linear(in_features=150, out_features=1, bias=True)\n  )\n  (sigmoid): Sigmoid()\n)"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resize_data = ResizeData(len_of_value=max_num_of_chars_in_value)\n",
    "value_to_one_hot = ValueToCharSequence(\n",
    "    len_of_value=max_num_of_chars_in_value)\n",
    "\n",
    "one_hot_encoding_of_type = OneHotEncodingOfType(max_types_to_select=10,\n",
    "                                                types_in_dataset_file_path=list_of_types_in_dataset_out_file)  # We select only top 10 types\n",
    "size_of_type_encoding = len(one_hot_encoding_of_type.one_hot_init)\n",
    "\n",
    "var_name_fastText_embd = fastTextEmbeddingOfVarName(embedding_path=token_embedding_path)\n",
    "embedding_dim = var_name_fastText_embd.embedding_dim\n",
    "\n",
    "len_repr = RepresentLen()\n",
    "shape_repr = RepresentShape()\n",
    "\n",
    "data_transformations = [resize_data,  # must be always the first transformation\n",
    "                        var_name_fastText_embd,\n",
    "                        value_to_one_hot,\n",
    "                        one_hot_encoding_of_type,\n",
    "                        len_repr,\n",
    "                        shape_repr\n",
    "                        ]\n",
    "\n",
    "model = VarValueClassifierRNN(embedding_dim=embedding_dim,\n",
    "                              num_of_characters_in_alphabet=value_to_one_hot.nbs_chars,\n",
    "                              model_name=model_name,\n",
    "                              size_of_value=resize_data.len_of_value)\n",
    "\n",
    "assert model is not None, \"Initialize a model to run training/testing\"\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Ablation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if len(features_to_ablate):\n",
    "    ablation_transformer = AblationTransformer(features_to_ablate=features_to_ablate)\n",
    "    print(f\"## Not using features --> {features_to_ablate} ##\")\n",
    "    data_transformations.append(ablation_transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- Reading dataset for training ---------------\n",
      "Few values from the dataset -->\n"
     ]
    },
    {
     "data": {
      "text/plain": "             file       var  \\\n0       nb_921790      seq1   \n1       nb_921790      seq1   \n2       nb_893796     total   \n3       nb_893796     total   \n4       nb_401647  colormap   \n...           ...       ...   \n980659  nb_326689   ourlist   \n980660  nb_668102      data   \n980661  nb_668102      data   \n980662  nb_707227      varX   \n980663  nb_707227      varX   \n\n                                                    value  line     type  \\\n0                                                    AACC    49      str   \n1                                                     100    49      int   \n2                                                       0    19      int   \n3       [-0.08338161 -0.07472355 -0.0661398  -0.057629...    19  ndarray   \n4                                  ['red' 'lime' 'black']   145  ndarray   \n...                                                   ...   ...      ...   \n980659                            timeseries/time-scales/   387      str   \n980660  [[ 0.  0.  5. 13.  9.  1.  0.  0.  0.  0.]\\n [...    55  ndarray   \n980661                                  62.38339475586176    55    float   \n980662                                                  0    81      int   \n980663  https://localhost:9443/resource/1.0.0/artifact...    81      str   \n\n         len       shape  p_buggy orig_type  \n0          4          -1      0.0       str  \n1         -1          -1      1.0       str  \n2         -1          -1      0.0       int  \n3         21       (21,)      1.0       int  \n4          3        (3,)      0.0   ndarray  \n...      ...         ...      ...       ...  \n980659    23          -1      1.0      list  \n980660  1797  (1797, 64)      0.0   ndarray  \n980661    -1          -1      1.0   ndarray  \n980662    -1          -1      0.0       int  \n980663    93          -1      1.0       int  \n\n[980664 rows x 9 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>file</th>\n      <th>var</th>\n      <th>value</th>\n      <th>line</th>\n      <th>type</th>\n      <th>len</th>\n      <th>shape</th>\n      <th>p_buggy</th>\n      <th>orig_type</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>nb_921790</td>\n      <td>seq1</td>\n      <td>AACC</td>\n      <td>49</td>\n      <td>str</td>\n      <td>4</td>\n      <td>-1</td>\n      <td>0.0</td>\n      <td>str</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>nb_921790</td>\n      <td>seq1</td>\n      <td>100</td>\n      <td>49</td>\n      <td>int</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>1.0</td>\n      <td>str</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>nb_893796</td>\n      <td>total</td>\n      <td>0</td>\n      <td>19</td>\n      <td>int</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>0.0</td>\n      <td>int</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>nb_893796</td>\n      <td>total</td>\n      <td>[-0.08338161 -0.07472355 -0.0661398  -0.057629...</td>\n      <td>19</td>\n      <td>ndarray</td>\n      <td>21</td>\n      <td>(21,)</td>\n      <td>1.0</td>\n      <td>int</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>nb_401647</td>\n      <td>colormap</td>\n      <td>['red' 'lime' 'black']</td>\n      <td>145</td>\n      <td>ndarray</td>\n      <td>3</td>\n      <td>(3,)</td>\n      <td>0.0</td>\n      <td>ndarray</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>980659</th>\n      <td>nb_326689</td>\n      <td>ourlist</td>\n      <td>timeseries/time-scales/</td>\n      <td>387</td>\n      <td>str</td>\n      <td>23</td>\n      <td>-1</td>\n      <td>1.0</td>\n      <td>list</td>\n    </tr>\n    <tr>\n      <th>980660</th>\n      <td>nb_668102</td>\n      <td>data</td>\n      <td>[[ 0.  0.  5. 13.  9.  1.  0.  0.  0.  0.]\\n [...</td>\n      <td>55</td>\n      <td>ndarray</td>\n      <td>1797</td>\n      <td>(1797, 64)</td>\n      <td>0.0</td>\n      <td>ndarray</td>\n    </tr>\n    <tr>\n      <th>980661</th>\n      <td>nb_668102</td>\n      <td>data</td>\n      <td>62.38339475586176</td>\n      <td>55</td>\n      <td>float</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>1.0</td>\n      <td>ndarray</td>\n    </tr>\n    <tr>\n      <th>980662</th>\n      <td>nb_707227</td>\n      <td>varX</td>\n      <td>0</td>\n      <td>81</td>\n      <td>int</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>0.0</td>\n      <td>int</td>\n    </tr>\n    <tr>\n      <th>980663</th>\n      <td>nb_707227</td>\n      <td>varX</td>\n      <td>https://localhost:9443/resource/1.0.0/artifact...</td>\n      <td>81</td>\n      <td>str</td>\n      <td>93</td>\n      <td>-1</td>\n      <td>1.0</td>\n      <td>int</td>\n    </tr>\n  </tbody>\n</table>\n<p>980664 rows × 9 columns</p>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using 980664 examples for training and validation which contains 490332 positive examples & 490332 negative examples\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='Running Epochs', max=15.0, style=ProgressStyle(descriptio…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2b311f22a9bd4c4db9bb03d8c65e2761"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch  2400/ 6129 |  t_loss        0.52 | \n",
      "Batch  4800/ 6129 |  t_loss        0.49 | \n",
      "Batch  1200/ 1532 |  v_loss        0.37 | \n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch      1/  15 | Training loss= 0.48 | Validation loss= 0.37 | lr=1.00 | fscore=0.84 | took=231.75 seconds\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Batch  2400/ 6129 |  t_loss        0.45 | \n",
      "Batch  4800/ 6129 |  t_loss        0.45 | \n",
      "Batch  1200/ 1532 |  v_loss        0.36 | \n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch      2/  15 | Training loss= 0.44 | Validation loss= 0.36 | lr=1.00 | fscore=0.83 | took=231.49 seconds\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Batch  2400/ 6129 |  t_loss        0.43 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-13:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.8/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/jibesh/.local/lib/python3.8/site-packages/torch/utils/data/_utils/pin_memory.py\", line 28, in _pin_memory_loop\n",
      "    r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/usr/local/lib/python3.8/multiprocessing/queues.py\", line 116, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/jibesh/.local/lib/python3.8/site-packages/torch/multiprocessing/reductions.py\", line 289, in rebuild_storage_fd\n",
      "    fd = df.detach()\n",
      "  File \"/usr/local/lib/python3.8/multiprocessing/resource_sharer.py\", line 57, in detach\n",
      "    with _resource_sharer.get_connection(self._id) as conn:\n",
      "  File \"/usr/local/lib/python3.8/multiprocessing/resource_sharer.py\", line 87, in get_connection\n",
      "    c = Client(address, authkey=process.current_process().authkey)\n",
      "  File \"/usr/local/lib/python3.8/multiprocessing/connection.py\", line 509, in Client\n",
      "    deliver_challenge(c, authkey)\n",
      "  File \"/usr/local/lib/python3.8/multiprocessing/connection.py\", line 740, in deliver_challenge\n",
      "    response = connection.recv_bytes(256)        # reject large message\n",
      "  File \"/usr/local/lib/python3.8/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/usr/local/lib/python3.8/multiprocessing/connection.py\", line 414, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/local/lib/python3.8/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_29310/331444392.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     12\u001B[0m         dataset=validation_dataset, batch_size=batch_size, shuffle=True, drop_last=True, **config)\n\u001B[1;32m     13\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 14\u001B[0;31m     model.run_epochs(training_data=train_data,\n\u001B[0m\u001B[1;32m     15\u001B[0m                      validation_data=validation_data, num_epochs=num_epochs, results_dir=results_dir)\n",
      "\u001B[0;32m~/nalin/src/nn/model.py\u001B[0m in \u001B[0;36mrun_epochs\u001B[0;34m(self, training_data, validation_data, num_epochs, results_dir)\u001B[0m\n\u001B[1;32m    189\u001B[0m             \u001B[0mstart_time\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtime\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    190\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 191\u001B[0;31m             \u001B[0mtraining_loss\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrun_training\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtraining_data\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    192\u001B[0m             validation_loss, list_of_actual_labels, list_of_predicted_labels = self.run_validation(\n\u001B[1;32m    193\u001B[0m                 data=validation_data)\n",
      "\u001B[0;32m~/nalin/src/nn/model.py\u001B[0m in \u001B[0;36mrun_training\u001B[0;34m(self, data)\u001B[0m\n\u001B[1;32m     79\u001B[0m                     \"Batch {:5d}/{:5d} |  t_loss {:11.2f} | \".format(i + 1, len(data), running_loss / (i + 1)))\n\u001B[1;32m     80\u001B[0m             \u001B[0mnn\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mutils\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mclip_grad_norm_\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mparameters\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m0.5\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 81\u001B[0;31m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0moptimizer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     82\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     83\u001B[0m         \u001B[0mavg_training_loss\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mround\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mrunning_loss\u001B[0m \u001B[0;34m/\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m2\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/torch/optim/optimizer.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     86\u001B[0m                 \u001B[0mprofile_name\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m\"Optimizer.step#{}.step\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mobj\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__class__\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__name__\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     87\u001B[0m                 \u001B[0;32mwith\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mautograd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprofiler\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrecord_function\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mprofile_name\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 88\u001B[0;31m                     \u001B[0;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     89\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mwrapper\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     90\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/torch/autograd/grad_mode.py\u001B[0m in \u001B[0;36mdecorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     26\u001B[0m         \u001B[0;32mdef\u001B[0m \u001B[0mdecorate_context\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     27\u001B[0m             \u001B[0;32mwith\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__class__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 28\u001B[0;31m                 \u001B[0;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     29\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mcast\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mF\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdecorate_context\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     30\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/torch/optim/adam.py\u001B[0m in \u001B[0;36mstep\u001B[0;34m(self, closure)\u001B[0m\n\u001B[1;32m    131\u001B[0m                     \u001B[0mstate_steps\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mstate\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'step'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    132\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 133\u001B[0;31m             F.adam(params_with_grad,\n\u001B[0m\u001B[1;32m    134\u001B[0m                    \u001B[0mgrads\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    135\u001B[0m                    \u001B[0mexp_avgs\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/torch/optim/_functional.py\u001B[0m in \u001B[0;36madam\u001B[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001B[0m\n\u001B[1;32m     85\u001B[0m         \u001B[0;31m# Decay the first and second moment running average coefficient\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     86\u001B[0m         \u001B[0mexp_avg\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmul_\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mbeta1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0madd_\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mgrad\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0malpha\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m1\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0mbeta1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 87\u001B[0;31m         \u001B[0mexp_avg_sq\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmul_\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mbeta2\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0maddcmul_\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mgrad\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgrad\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconj\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvalue\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m1\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0mbeta2\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     88\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mamsgrad\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     89\u001B[0m             \u001B[0;31m# Maintains the maximum of all 2nd moment running avg. till now\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "if train:\n",
    "    print(f\"{'-' * 15} Reading dataset for training {'-' * 15}\")\n",
    "    # Read the dataset\n",
    "    training_dataset, validation_dataset = get_training_val_dataset(\n",
    "        positive_examples_dataset_file_path=pos_dataset_file_path,\n",
    "        negative_examples_dataset_file_path=neg_dataset_file_path,\n",
    "        all_transformations=data_transformations,\n",
    "        nb_examples=-1)\n",
    "    train_data = DataLoader(\n",
    "        dataset=training_dataset, batch_size=batch_size, shuffle=True, drop_last=True, **config)\n",
    "    validation_data = DataLoader(\n",
    "        dataset=validation_dataset, batch_size=batch_size, shuffle=True, drop_last=True, **config)\n",
    "\n",
    "    model.run_epochs(training_data=train_data,\n",
    "                     validation_data=validation_data, num_epochs=num_epochs, results_dir=results_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if test:\n",
    "    print(f\"{'-' * 15} Reading dataset for testing {'-' * 15}\")\n",
    "    test_dataset = get_test_dataset(\n",
    "        test_examples_dir=test_examples_dir,\n",
    "        results_dir=results_dir,\n",
    "        all_transformations=data_transformations,\n",
    "        dataset_out_file=test_dataset_file_path)\n",
    "    batched_test_dataset = DataLoader(\n",
    "        dataset=test_dataset, batch_size=batch_size, shuffle=False, drop_last=False, **config)\n",
    "    model.load_model(path_to_saved_model=saved_model_path)\n",
    "    predictions = model.run_testing(data=batched_test_dataset)\n",
    "\n",
    "    test_data_with_predictions = test_dataset.data\n",
    "    test_data_with_predictions['predicted_p_buggy'] = predictions\n",
    "\n",
    "    predicted_outfile_path = os.path.join(results_dir,\n",
    "                                          f'prediction_results/{Path(test_dataset_file_path).stem}_predictions.pkl')\n",
    "    print(f\"Writing to '{predicted_outfile_path}'\")\n",
    "    test_data_with_predictions.sort_values('predicted_p_buggy', ascending=False, inplace=True)\n",
    "    test_data_with_predictions.reset_index(drop=True, inplace=True)\n",
    "    # print(\n",
    "    #     f\"\\n Prediction results is follows: \\n\\n{test_data['predicted_p_buggy'].value_counts()}\")\n",
    "\n",
    "    # test_data_with_predictions.to_csv(predicted_outfile_path)\n",
    "    test_data_with_predictions.to_pickle(path=predicted_outfile_path, compression='gzip')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f2ac3815f490baa7abc22c2717d4dc21d64c0b7e1df0d49128b5f181d05973e7"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}